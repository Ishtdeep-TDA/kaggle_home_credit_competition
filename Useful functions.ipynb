{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean encoding\n",
    "# this first dummifies the target then creates mean encodings.\n",
    "def mean_encoding(columns,df,target,df_test): # here columns is the name of the columns to encode and df is the dataframe target is the column name for the target variable\n",
    "    mean_en = pd.DataFrame()\n",
    "    mean_en_test = pd.DataFrame()\n",
    "    temp = pd.DataFrame(df[target])\n",
    "    n = temp[target].nunique()\n",
    "    un = temp[target].unique()\n",
    "    count = 0\n",
    "    new_target = pd.DataFrame()\n",
    "    for i in range(n):\n",
    "        tempx = []\n",
    "        for j in range(df.shape[0]):\n",
    "            if un[i] == temp.iloc[j,0]:\n",
    "                tempx.append(1)\n",
    "            else:\n",
    "                tempx.append(0)\n",
    "        new_target[un[i]] = tempx\n",
    "    df = df.drop(target,axis = 1)\n",
    "    temp2 = pd.concat([df,new_target],axis = 1)\n",
    "    for i in columns:\n",
    "        for j in range(n):\n",
    "            temp = temp2.groupby(i)[un[j]].mean()\n",
    "            mean_en[f\"{i}+{un[j]}_mean_encoding\"] = temp2[i].map(temp)\n",
    "            mean_en_test[f\"{i}+{un[j]}_mean_encoding\"] = df_test[i].map(temp)\n",
    "            if mean_en[f\"{i}+{un[j]}_mean_encoding\"].isnull().sum()>0:\n",
    "                glob_mean = temp2[un[j]].mean()\n",
    "                mean_en[f\"{i}+{un[j]}_mean_encoding\"] = mean_en[f\"{i}+{un[j]}_mean_encoding\"].fillna(glob_mean)\n",
    "                mean_en_test[f\"{i}+{un[j]}_mean_encoding\"] = mean_en_test[f\"{i}+{un[j]}_mean_encoding\"].fillna(glob_mean)\n",
    "    return mean_en,mean_en_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOMATED TRAINING FUNCTIONS\n",
    "def train_lgbm_mean(df_train,df_test,lgbm_para,cols,ytrain,target_col_name):\n",
    "    \"\"\"\n",
    "    df_train:dataframe to train\n",
    "    df_test:test dataframe\n",
    "    lgbm_para:all the parameters required for the lightgbm\n",
    "    cols:the columns for which you would like to do mean encoding\n",
    "    ytrain: target for training data\n",
    "    target_col_name:target column name\n",
    "    \n",
    "    \n",
    "    To run this convert and convert2 functions must be there\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    gbm = lightgbm\n",
    "    \n",
    "    columns_encode = [\"Violations\",\"Accident_Type_Code\",\"Days_Since_Inspection\",\"Total_Safety_Complaints\"]\n",
    "\n",
    "    df_cols = list(df_train.columns)   \n",
    "\n",
    "    temp_df = pd.concat([df_train,ytrain],axis = 1)\n",
    "    print(temp_df.shape)\n",
    "    df_cols.append(target_col_name)\n",
    "\n",
    "    temp_df.columns = df_cols\n",
    "\n",
    "    x,z = mean_encoding(cols,temp_df,target_col_name,df_test)\n",
    "\n",
    "    y_c = convert2(y)\n",
    "    \n",
    "    train = pd.concat([temp_df,x],axis = 1)\n",
    "    test = pd.concat([df_test,z],axis = 1)\n",
    "    train = train.drop(target_col_name,axis =1)\n",
    "    train_d = gbm.Dataset(train.values,label = y_c)\n",
    "    lgbm = gbm.train(lgbm_para,train_set=train_d)\n",
    "    \n",
    "    return lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(df_train,lgbm_para,ytrain):\n",
    "    \"\"\"\n",
    "    df_train:dataframe to train\n",
    "    df_test:test dataframe\n",
    "    lgbm_para:all the parameters required for the lightgbm\n",
    "    cols:the columns for which you would like to do mean encoding\n",
    "    ytrain: target for training data\n",
    "    target_col_name:target column name\n",
    "    \n",
    "    \n",
    "    To run this convert and convert2 functions must be there\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    gbm = lightgbm\n",
    "    y_c = convert2(y)\n",
    "    train_d = gbm.Dataset(df_train.values,label = y_c)\n",
    "    lgbm = gbm.train(lgbm_para,train_set=train_d)\n",
    "    \n",
    "    return lgbm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(params,df,y_nn):\n",
    "    \n",
    "    reg = l2(params[\"reg\"])\n",
    "    model = Sequential()\n",
    "    model.add(Dense(141,activation=tf.nn.relu ,kernel_initializer='lecun_uniform',kernel_regularizer = reg))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1000,activation=tf.nn.relu ,kernel_initializer='lecun_uniform',kernel_regularizer = reg))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1000,activation=tf.nn.relu ,kernel_initializer='lecun_uniform',kernel_regularizer = reg))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1000,activation=tf.nn.relu ,kernel_initializer='lecun_uniform',kernel_regularizer =reg))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1000,activation=tf.nn.relu ,kernel_initializer='lecun_uniform',kernel_regularizer = reg))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(500,activation=tf.nn.relu ,kernel_initializer='lecun_uniform',kernel_regularizer = reg))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(4,activation=tf.nn.softmax))\n",
    "    \n",
    "    adam = keras.optimizers.Adam(learning_rate=params[\"learning_rate\"], beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "    mc = ModelCheckpoint(r'E:\\Data Science\\Kaggle Competitions dataset\\Hackerearth Airplane Accident/best_model.h5', monitor='categorical_crossentropy', mode='min', verbose=1, save_best_only=True)        \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='categorical_crossentropy',mode = \"min\", factor=0.params[\"factor\"],patience=params[\"patience\"], min_lr=0.000001)\n",
    "    \n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy',metrics=['categorical_crossentropy'])\n",
    "\n",
    "    history = model.fit(df,y_nn.values, epochs=params[\"epochs\"],batch_size = params[\"batch_size\"],callbacks = [mc,reduce_lr],verbose =1)\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf(params,df,y):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_jobs = -1,n_estimators = params[\"n_estimators\"],min_samples_split = params[\"min_samples_split\"],max_depth = params[\"max_depth\"],max_features = params[\"max_features\"])\n",
    "    rf.fit(df,y)\n",
    "    \n",
    "    return rf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LR(params,df,y):\n",
    "    \n",
    "    log_reg = LogisticRegression(solver = \"sag\",max_iter=params[\"max_iter\"],C=params[\"C\"],penalty=\"l2\",multi_class=\"ovr\",n_jobs=8)\n",
    "    log_reg.fit(df,y)\n",
    "    return log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_KNN(params,df,y):\n",
    "    knn = KNeighborsClassifier(n_neighbors = params[\"n_neighbors\"],n_jobs = -1)\n",
    "    knn.fit(df,y)\n",
    "    \n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gnb(params,df,y):\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(df,y)\n",
    "    \n",
    "    return gnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCtion to extract feature importances\n",
    "\n",
    "imp_feat = lgbm_test.feature_importance(importance_type = \"gain\") # these are the feature importances\n",
    "feat_name = lgbm_test.feature_name() # these are the feature names\n",
    "\n",
    "# lets make a dictionary\n",
    "\n",
    "feat_dict = {}\n",
    "for c,i in enumerate(imp_feat):\n",
    "    feat_dict[i] = feat_name[c]\n",
    "z = sorted(feat_dict.keys(),reverse = True)\n",
    "count = 0\n",
    "important_features = [] #this contains all the important features names in a list\n",
    "for i in z:\n",
    "    if count<50: # number of features to extract\n",
    "        important_features.append(feat_dict[i])\n",
    "    else:\n",
    "        break\n",
    "    count =count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KERAS TRANSFERRING WEIGHTS OF 1 LAYER TO ANOTHER\n",
    "# here we get weights from saved_auto and map it to temp\n",
    "for c,i in enumerate(temp.layers):\n",
    "    temp.layers[c].set_weights(saved_auto.layers[c].get_weights())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO INSTALL EFFICIENT NET\n",
    "pip install keras_efficientnets #(EASIEST WAY)\n",
    "# also look at efficient net from pip can be a seperate module idk why\n",
    "from keras_efficientnets import EfficientNetB5\n",
    "effnet = EfficientNetB5(input_shape=(456,456,3),\n",
    "                        weights='imagenet',\n",
    "                        include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For installing efficientnet in tensorflow 2.0\n",
    "\n",
    "pip install -U efficientnet\n",
    "\n",
    "import efficientnet.tfkeras as efn \n",
    "effnet = efn.EfficientNetB3(input_shape = (300,300,3),weights='imagenet',include_top = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV K fold\n",
    "cv = KFold(4, random_state=42,shuffle = True)\n",
    "\n",
    "for train, test in cv.split(normal):\n",
    "    X_train, X_test = normal[train], normal[test]\n",
    "    y_train, y_test = y_nn.values[train], y_nn.values[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K fold\n",
    "cv = StratifiedKFold(4, random_state=42,shuffle = True)\n",
    "\n",
    "for train, test in cv.split(normal,temp_y):\n",
    "    X_train, X_test = normal[train], normal[test]\n",
    "    y_train, y_test = y_nn.values[train], y_nn.values[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Outliers\n",
    "\n",
    "# METHOD 1\n",
    "import seaborn as sns\n",
    "sns.boxplot(x=boston_df['DIS']) # This is a boxplot Anything that appears to be a point in the plot is considered as an outlier\n",
    "\n",
    "# METHOD 2\n",
    "# Removing outliers\n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "z= []\n",
    "for i in feat_df.columns:\n",
    "    temp = np.abs(stats.zscore(feat_df[i])) #This gives the z scores of the particular df\n",
    "    z.append(temp)\n",
    "\n",
    "# Z Scores that are above or below 3,-3 are considered to be outliers\n",
    "# Z score kind of measures how many standard deviations is a point far of\n",
    "to_del = {0} # rows to be deleted\n",
    "\n",
    "# The code below deletes the outliers\n",
    "# deletion is not always recommended capping can also be used instead of deletion\n",
    "for i in (z):\n",
    "    for c,j in enumerate(i):\n",
    "        if j >3:\n",
    "            to_del.add(c) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFECT FUNCTION FOR MEAN target ENCODING\n",
    "\n",
    "# this first dummifies the target then creates mean encodings for both train and test\n",
    "def mean_encoding(columns,df,target,df_test): # here columns is the name of the columns to encode and df is the dataframe target is the column name for the target variable\n",
    "    mean_en = pd.DataFrame()\n",
    "    mean_en_test = pd.DataFrame()\n",
    "    temp = pd.DataFrame(df[target])\n",
    "    n = temp[target].nunique()\n",
    "    un = temp[target].unique()\n",
    "    count = 0\n",
    "    new_target = pd.DataFrame()\n",
    "    for i in range(n):\n",
    "        tempx = []\n",
    "        for j in range(df.shape[0]):\n",
    "            if un[i] == temp.iloc[j,0]:\n",
    "                tempx.append(1)\n",
    "            else:\n",
    "                tempx.append(0)\n",
    "        new_target[un[i]] = tempx\n",
    "    df = df.drop(target,axis = 1)\n",
    "    temp2 = pd.concat([df,new_target],axis = 1)\n",
    "    for i in columns:\n",
    "        for j in range(n):\n",
    "            temp = temp2.groupby(i)[un[j]].mean()\n",
    "            mean_en[f\"{i}+{un[j]}_mean_encoding\"] = temp2[i].map(temp)\n",
    "            mean_en_test[f\"{i}+{un[j]}_mean_encoding\"] = df_test[i].map(temp)\n",
    "            if mean_en[f\"{i}+{un[j]}_mean_encoding\"].isnull().sum()>0:\n",
    "                glob_mean = temp2[un[j]].mean()\n",
    "                mean_en[f\"{i}+{un[j]}_mean_encoding\"] = mean_en[f\"{i}+{un[j]}_mean_encoding\"].fillna(glob_mean)\n",
    "                mean_en_test[f\"{i}+{un[j]}_mean_encoding\"] = mean_en_test[f\"{i}+{un[j]}_mean_encoding\"].fillna(glob_mean)\n",
    "    return mean_en,mean_en_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROSS VALIDATION WITH MEAN ENCODING\n",
    "\n",
    "cv = StratifiedKFold(4, random_state=42,shuffle = True)\n",
    "li =[]\n",
    "for train, test in cv.split(df_feat,y):\n",
    "    X_train, X_test = df_feat.values[train], df_feat.values[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    y_c = convert2(y_train)\n",
    "    y_c_t = convert2(y_test)\n",
    "    \n",
    "    df_cols = list(df_feat.columns)\n",
    "    \n",
    "    temp_train = pd.DataFrame(X_train)\n",
    "    temp_train.columns = df_cols\n",
    "    temp_y = pd.DataFrame(y_train)\n",
    "    temp_y.columns = [\"Severity\"]\n",
    "    temp_test = pd.DataFrame(X_test)\n",
    "    temp_test.columns = df_cols\n",
    "    \n",
    "    columns_encode = [\"Violations\",\"Accident_Type_Code\",\"Days_Since_Inspection\",\"Total_Safety_Complaints\"]\n",
    "    \n",
    "    temp_df = pd.DataFrame(np.hstack([temp_train,temp_y]))\n",
    "    df_cols.append(\"Severity\")\n",
    "    temp_df.columns = df_cols\n",
    "    x,z = mean_encoding(columns_encode,temp_df,\"Severity\",temp_test)\n",
    "    \n",
    "    train = pd.concat([temp_train,x],axis = 1)\n",
    "    test = pd.concat([temp_test,z],axis = 1)\n",
    "    #train = train.drop(\"Severity\",axis =1)\n",
    "    train_d = gbm.Dataset(train.values,label = y_c)\n",
    "    test_d= gbm.Dataset(test.values,label = y_c_t)\n",
    "    lgbm = gbm.train(all_params,train_set=train_d,valid_sets=test_d)\n",
    "    temp = lgbm.predict(test.values)\n",
    "    li.append(f1_score(y_c_t,convert2(convert(temp)),average = \"weighted\"))\n",
    "    temp1 = f1_score(y_c_t,convert2(convert(temp)),average = \"weighted\")\n",
    "output = (sum(li)/4)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ENGINEERING # GENERATING A LOT OF RANDOM FEATURES\n",
    "groupby_feat = pd.DataFrame()\n",
    "cat_col = [\"name_of_drug\",\"use_case_for_drug\"] #categorical columns for which we want to create features\n",
    "num_col = [\"number_of_times_prescribed\",\"effectiveness_rating\"] #Numerical columns for which we want to create features for the categorical columns\n",
    "aggs = [np.mean,max,min,np.std,sum] #functions we want to apply for the columns\n",
    "fun_name_dict = {\n",
    "    np.mean:'mean',\n",
    "    max:'max',\n",
    "    min:'min',\n",
    "    np.std:'std',\n",
    "    sum:'sum'\n",
    "}\n",
    "\n",
    "for cat in cat_col:\n",
    "    for num in num_col:\n",
    "        for fun in aggs:\n",
    "            fun1 = fun_name_dict[fun]\n",
    "            groupby_feat[f'feature_{cat}_{num}_{fun1}'] = df.groupby(cat)[num].transform(fun)\n",
    "\n",
    "            # FOR BETTER SEE GENERATING RANDOM FEATURES IN KAGGLE PREDICTING MOLECULAR PROPERTIES CHALLENGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random feature functions in Cross Validation\n",
    "# used for creating features and mapping them to test data in the cv\n",
    "def create_features(df):\n",
    "    groupby_feat = pd.DataFrame()\n",
    "    cat_col = [\"name_of_drug\",\"use_case_for_drug\"] #categorical columns for which we want to create features\n",
    "    num_col = [\"number_of_times_prescribed\",\"effectiveness_rating\"] #Numerical columns for which we want to create features for the categorical columns\n",
    "    aggs = [np.mean,max,min,np.std,sum] #functions we want to apply for the columns\n",
    "    fun_name_dict = {\n",
    "        np.mean:'mean',\n",
    "        max:'max',\n",
    "        min:'min',\n",
    "        np.std:'std',\n",
    "        sum:'sum'\n",
    "    }\n",
    "    feat_dict = {}\n",
    "    \n",
    "    for cat in cat_col:\n",
    "        for num in num_col:\n",
    "            for fun in aggs:\n",
    "                fun1 = fun_name_dict[fun]\n",
    "                groupby_feat[f'feature_{cat}_{num}_{fun1}'] = df.groupby(cat)[num].transform(fun)\n",
    "                feat_dict[f'feature_{cat}_{num}_{fun1}'] = df.groupby(cat)[num].apply(fun)\n",
    "    diff = (pd.to_datetime('01-Jan-20', format='%d-%b-%y') - pd.to_datetime(df['drug_approved_by_UIC'], format='%d-%b-%y')).dt.days\n",
    "   \n",
    "    groupby_feat['time_reference'] = diff\n",
    "    all_feat = pd.concat([df,groupby_feat],axis = 1)\n",
    "    \n",
    "    return all_feat,feat_dict,cat_col\n",
    "\n",
    "import re\n",
    "def map_features(data,feat_dictionary,cat): # used for mapping features to test data\n",
    "    for i in feat_dictionary.keys():\n",
    "        for j in cat:\n",
    "            if re.search(j,i):\n",
    "                temp_dict_1 = dict(zip(feat_dictionary[i].index, feat_dictionary[i].values))\n",
    "                data[i] = data[j].map(temp_dict_1)\n",
    "                print(j,i)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting features\n",
    "# line features Safety score and days since inspection\n",
    "# making graph\n",
    "# variables to output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "r = 5 # rows\n",
    "c=5 #columns\n",
    "t = 0 #temporary counter\n",
    "f = plt.figure(figsize=(60,60))\n",
    "gs = f.add_gridspec(r,c)\n",
    "count = 0\n",
    "colors = ['red','green','blue','yellow']\n",
    "\n",
    "for i in range(len(df.columns)):\n",
    "    for j in range(i,len(df.columns)):\n",
    "        if count%r==0:\n",
    "            t=t+1\n",
    "        f.add_subplot(gs[count%r,t])\n",
    "        plt.scatter(df[df.columns[i]],df[df.columns[j]],c=y_conv, cmap=ListedColormap(colors))\n",
    "        plt.title(f'X = {df.columns[i]}+ Y ={df.columns[j]}')\n",
    "        count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPEROPT\n",
    "from hyperopt import hp, tpe, fmin\n",
    "\n",
    "fixed_param = {\n",
    "               \"objective\":\"multiclass\",\n",
    "               \"metric\":\"multi_logloss\",\n",
    "               \"num_class\":4,\n",
    "               \"boosting_type\":\"gbdt\"\n",
    "              }\n",
    "\n",
    "# This is the space that you want this function to explore\n",
    "space_gbm = {\n",
    "             'max_depth': hp.uniform('max_depth',5,40),\n",
    "             'num_iterations': hp.uniform('num_iterations',50, 600),\n",
    "             'num_leaves': hp.uniform('num_leaves',10, 120),\n",
    "             'min_data_in_leaf':hp.uniform('min_data_in_leaf',50, 500),\n",
    "             'learning_rate': hp.uniform('learning_rate',0.01, 0.1)\n",
    "            }\n",
    "# THIS is the objective function that we want to minimize (note that we are multiplying the output by -1 as this function is always minimized never maximized)\n",
    "def objective_gbm(params):\n",
    "    cv = StratifiedKFold(4, random_state=42,shuffle = True)\n",
    "    li =[]\n",
    "    # THis is CV in hyperopt\n",
    "    for train, test in cv.split(df,y):\n",
    "        X_train, X_test = df.values[train], df.values[test]\n",
    "        y_train, y_test = y.values[train], y.values[test]\n",
    "        y_c = convert2(y_train)\n",
    "        y_c_t = convert2(y_test)\n",
    "    \n",
    "        params = {'num_iterations': int(params['num_iterations']),'max_depth': int(params['max_depth']),'num_leaves': int(params['num_leaves']),'min_data_in_leaf': int(params['min_data_in_leaf']),'learning_rate': float(params['learning_rate'])}\n",
    "        all_params = fixed_param\n",
    "        for i in params:\n",
    "            all_params[i] = params[i]\n",
    "    \n",
    "        train_d = gbm.Dataset(X_train,label = y_c)\n",
    "        test_d= gbm.Dataset(X_test,label = y_c_t)\n",
    "        lgbm = gbm.train(all_params,train_set=train_d,valid_sets=test_d)\n",
    "        temp = lgbm.predict(X_test)\n",
    "        li.append(f1_score(y_c_t,convert2(convert(temp)),average = \"weighted\"))\n",
    "        temp1 = f1_score(y_c_t,convert2(convert(temp)),average = \"weighted\")\n",
    "    output = (sum(li)/4)\n",
    "    print(output)\n",
    "    return -1*output\n",
    "# THis is the actual function that is run by this function\n",
    "best_gbm = fmin(objective_gbm,\n",
    "    space=space_gbm,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL FEATURE ENGINEERING FUNCTION\n",
    "def create_features_full(df):\n",
    "    df['molecule_couples'] = df.groupby('molecule_name')['id'].transform('count')\n",
    "    df['molecule_dist_mean'] = df.groupby('molecule_name')['dist'].transform('mean')\n",
    "    df['molecule_dist_min'] = df.groupby('molecule_name')['dist'].transform('min')\n",
    "    df['molecule_dist_max'] = df.groupby('molecule_name')['dist'].transform('max')\n",
    "    df['molecule_dist_std'] = df.groupby('molecule_name')['dist'].transform('std')\n",
    "    df['atom_0_couples_count'] = df.groupby(['molecule_name', 'atom_index_0'])['id'].transform('count')\n",
    "    df['atom_1_couples_count'] = df.groupby(['molecule_name', 'atom_index_1'])['id'].transform('count')\n",
    "\n",
    "    num_cols = ['x_1', 'y_1', 'z_1', 'dist', 'dist_x', 'dist_y', 'dist_z'] # numeric columns\n",
    "    cat_cols = ['atom_index_0', 'atom_index_1', 'type', 'atom_1', 'type_0'] # categorical columns\n",
    "    aggs = ['mean', 'max', 'std', 'min']\n",
    "    for col in cat_cols:\n",
    "        df[f'molecule_{col}_count'] = df.groupby('molecule_name')[col].transform('count')\n",
    "\n",
    "    for cat_col in tqdm_notebook(cat_cols):\n",
    "        for num_col in num_cols:\n",
    "            for agg in aggs:\n",
    "                df[f'molecule_{cat_col}_{num_col}_{agg}'] = df.groupby(['molecule_name', cat_col])[num_col].transform(agg)\n",
    "                df[f'molecule_{cat_col}_{num_col}_{agg}_diff'] = df[f'molecule_{cat_col}_{num_col}_{agg}'] - df[num_col]\n",
    "                df[f'molecule_{cat_col}_{num_col}_{agg}_div'] = df[f'molecule_{cat_col}_{num_col}_{agg}'] / df[num_col]\n",
    "\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#csv download link\n",
    "from IPython.display import HTML\n",
    "\n",
    "def create_download_link(title = \"Download CSV file\", filename = \"linear_prediction.csv\"):  \n",
    "    html = '<a href={filename}>{title}</a>'\n",
    "    html = html.format(title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "# create a link to download the dataframe which was saved with .to_csv method\n",
    "create_download_link(filename='linear_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing DATA FOR MODE (FILLING NAN's BY THE MODE)\n",
    "df[i] = df[i].fillna(df[i].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "LDA = LinearDiscriminantAnalysis(n_components = 50)\n",
    "LDA.fit(normal_train,y_normal_train)\n",
    "LDA_data = LDA.transform(normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING A NN MODEL THROUGH KERAS\n",
    "model_json = model4.to_json()\n",
    "with open(\"overfitNN.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model4.save_weights(\"overfitNN.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(normal, y_normal, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model storing through pickle\n",
    "filename = \"Linear_regression.sav\"\n",
    "pickle.dump(Lreg, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVED MODEL DOWNLOAD\n",
    "def create_download_link(title = \"Download Model\", filename = \"Linear_regression.sav\"):  \n",
    "    html = '<a href={filename}>{title}</a>'\n",
    "    html = html.format(title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "# create a link to download the dataframe which was saved with .to_csv method\n",
    "create_download_link(filename='Linear_regression.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Neural Network\n",
    "from keras.regularizers import l2 as reg2,l1 as reg1\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,BatchNormalization,Dropout\n",
    "\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(200,activation=tf.nn.relu ,kernel_initializer='lecun_uniform'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dense(10000,activation=tf.nn.tanh ,kernel_initializer='lecun_uniform',kernel_regularizer=reg2))\n",
    "model4.add(Dropout(rate = 0.2))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dense(2000,activation=tf.nn.relu ,kernel_initializer='lecun_uniform',kernel_regularizer=reg2))\n",
    "model4.add(Dropout(rate = 0.2))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dense(1000,activation=tf.nn.tanh ,kernel_initializer='lecun_uniform',kernel_regularizer=reg2))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dense(500,activation=tf.nn.relu ,kernel_initializer='lecun_uniform',kernel_regularizer=reg2))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dense(100,activation=tf.nn.tanh ,kernel_initializer='lecun_uniform',kernel_regularizer=reg2))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dense(100,activation=tf.nn.relu ,kernel_initializer='lecun_uniform',kernel_regularizer=reg2))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dense(100,activation=tf.nn.relu ,kernel_initializer='lecun_uniform',kernel_regularizer=reg2))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dense(1,activation=tf.nn.sigmoid ,kernel_initializer='lecun_uniform'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history4 = model4.fit(X_train, y_train.values, epochs=30,batch_size = 1024,validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USEFUL TRANSFER LEARNING FUNCTIONS\n",
    "#picking specific layers and weights of a network\n",
    "pre_trained = []\n",
    "for i in automodel.layers[:8]:\n",
    "    pre_trained.append(i)\n",
    "\n",
    "# adding layers of pre_trained to new model\n",
    "new_model = Sequential()\n",
    "for i in pre_trained:\n",
    "    new_model.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freezing layers\n",
    "for layer in new_model.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting loss\n",
    "\n",
    "plt.plot(history4.history['loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting ROC\n",
    "#ROC defines how good the model is at separating classes\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thr = roc_curve(y_train, model4.predict_proba(X_train))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Plot for training data')\n",
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOME CNN FUNCTIONS\n",
    "\n",
    "\n",
    "#this function is useful for getting predictions after creating a model with a specific generator\n",
    "def get_preds_and_labels(model, generator):\n",
    "    \"\"\"\n",
    "    Get predictions and labels from the generator\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for _ in range(int(np.ceil(generator.samples / BATCH_SIZE))):\n",
    "        x, y = next(generator)\n",
    "        preds.append(model.predict(x))\n",
    "        labels.append(y)\n",
    "    # Flatten list of numpy arrays\n",
    "    return np.concatenate(preds).ravel(), np.concatenate(labels).ravel()\n",
    "\n",
    "\n",
    "\n",
    "# This is the metrics class and it has functions for keras callback \n",
    "#on train begin happens at the beginning of training \n",
    "# on epoch end happens at the end of each epoch\n",
    "#in this on epoch end function we save the model if the validation score improves\n",
    "# for the specified metric (in this case kappa)\n",
    "class Metrics(Callback):\n",
    "    \"\"\"\n",
    "    A custom Keras callback for saving the best model\n",
    "    according to the Quadratic Weighted Kappa (QWK) metric\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        \"\"\"\n",
    "        Initialize list of QWK scores on validation data\n",
    "        \"\"\"\n",
    "        self.val_kappas = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \"\"\"\n",
    "        Gets QWK score on the validation data\n",
    "        \"\"\"\n",
    "        # Get predictions and convert to integers\n",
    "        y_pred, labels = get_preds_and_labels(model, val_generator)\n",
    "        y_pred = np.rint(y_pred).astype(np.uint8).clip(0, 4)\n",
    "        # We can use sklearns implementation of QWK straight out of the box\n",
    "        # as long as we specify weights as 'quadratic'\n",
    "        _val_kappa = cohen_kappa_score(labels, y_pred, weights='quadratic')\n",
    "        self.val_kappas.append(_val_kappa)\n",
    "        print(f\"val_kappa: {round(_val_kappa, 4)}\")\n",
    "        if _val_kappa == max(self.val_kappas):\n",
    "            print(\"Validation Kappa has improved. Saving model.\")\n",
    "            self.model.save(SAVED_MODEL_NAME)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some pre processing of the images\n",
    "\n",
    "# to resize the image for a specific size\n",
    "image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "# To add gaussian noise (it somehow makes the model rhobust)\n",
    "image = cv2.addWeighted (image,4, cv2.GaussianBlur(image, (0,0) ,sigmaX), -4, 128)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideally we should make a function for pre processing which would pre process\n",
    "# the images with diff pre processings. This should be done to be able to use\n",
    "# the pre processing function in the generator\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    image = cv2.addWeighted (image,4, cv2.GaussianBlur(image, (0,0) ,sigmaX), -4, 128)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "#These are very important functions for data augmentation and generation while using\n",
    "# in the training and testing\n",
    "# Add Image augmentation to our generator\n",
    "# see ImageDataGenerator keras documentation\n",
    "train_datagen = ImageDataGenerator(rotation_range=360,\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                   validation_split=0.15,\n",
    "                                   preprocessing_function=preprocess_image, #this is the pre-processing function that will be applied to the image\n",
    "                                   rescale=1 / 128.)\n",
    "\n",
    "# Use the dataframe to define train and validation generators\n",
    "train_generator = train_datagen.flow_from_dataframe(train_df, \n",
    "                                                    x_col='id_code', \n",
    "                                                    y_col='diagnosis',\n",
    "                                                    directory = TRAIN_IMG_PATH,\n",
    "                                                    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    class_mode='other', \n",
    "                                                    subset='training')\n",
    "\n",
    "val_generator = train_datagen.flow_from_dataframe(train_df, \n",
    "                                                  x_col='id_code', \n",
    "                                                  y_col='diagnosis',\n",
    "                                                  directory = TRAIN_IMG_PATH,\n",
    "                                                  target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  class_mode='other',\n",
    "                                                  subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL MODEL HERE with early stopping and some other stuff that is put into\n",
    "# callbacks.\n",
    "\n",
    "# For tracking Quadratic Weighted Kappa score\n",
    "kappa_metrics = Metrics()\n",
    "# Monitor MSE to avoid overfitting and save best model\n",
    "es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=12)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                        factor=0.5, \n",
    "                        patience=4, \n",
    "                        verbose=1, \n",
    "                        mode='auto', \n",
    "                        epsilon=0.0001)\n",
    "\n",
    "# Begin training\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "                    epochs=35,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps = val_generator.samples // BATCH_SIZE,\n",
    "                    callbacks=[kappa_metrics, es, rlr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GLOVE_DIM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3a1b1fcdbb81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mglove_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'glove.twitter.27B.'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGLOVE_DIM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'd.txt'\u001b[0m \u001b[0;31m#the embedding file name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mglove_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'glove-global-vectors-for-word-representation/'\u001b[0m \u001b[0;31m# the directory where the embedding is stored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0memb_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GLOVE_DIM' is not defined"
     ]
    }
   ],
   "source": [
    "# for using EMBEDDINGS learned on some corpus #TRANSFER LEARNING\n",
    "# use this function for mapping the words to the embeddings.\n",
    "# Taken from https://www.kaggle.com/bertcarremans/using-word-embeddings-for-sentiment-analysis\n",
    "# just see this link it is flawless \n",
    "\n",
    "#Using pre-trained word embeddings\n",
    "#Because the training data is not so big, \n",
    "#the model might not be able to learn good embeddings for the sentiment analysis. Luckily we can load pre-trained word embeddings built on a much larger training data.\n",
    "#The GloVe database contains multiple pre-trained word embeddings, and more specific embeddings trained on tweets.#\n",
    "\n",
    "\n",
    "glove_file = 'glove.twitter.27B.' + str(GLOVE_DIM) + 'd.txt' #the embedding file name\n",
    "glove_dir = 'glove-global-vectors-for-word-representation/' # the directory where the embedding is stored\n",
    "emb_dict = {}\n",
    "glove = open(input_path / glove_dir / glove_file)\n",
    "for line in glove:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    emb_dict[word] = vector\n",
    "glove.close()\n",
    "\n",
    "#To feed this into an Embedding layer, we need to build a matrix containing the\n",
    "#words in the tweets and their representative word embedding. \n",
    "#So this matrix will be of shape (NB_WORDS, GLOVE_DIM)\n",
    "\n",
    "#Before using embeddings we must first make a map for tokenization of the data\n",
    "#This means that each word will be replaced by a token number which will uniquely\n",
    "#represent that word with a unique token\n",
    "tk = Tokenizer(num_words=NB_WORDS,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True,\n",
    "               split=\" \")\n",
    "tk.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tk.texts_to_sequences(X_train)\n",
    "X_test_seq = tk.texts_to_sequences(X_test)\n",
    "\n",
    "\n",
    "#ONLY USE THE BELOW FUNCTION AFTER TOKENIZATION\n",
    "\n",
    "emb_matrix = np.zeros((NB_WORDS, GLOVE_DIM))\n",
    "\n",
    "for w, i in tk.word_index.items():\n",
    "    # The word_index contains a token for all words of the training data so we need to limit that\n",
    "    if i < NB_WORDS:\n",
    "        vect = emb_dict.get(w)\n",
    "        # Check if the word from the training data occurs in the GloVe word embeddings\n",
    "        # Otherwise the vector is kept with only zeros\n",
    "        if vect is not None:\n",
    "            emb_matrix[i] = vect\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Below is a sample model We can replace with our own model.\n",
    "\n",
    "glove_model = models.Sequential()\n",
    "glove_model.add(layers.Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN)) #nb words is the number of words (size of dictionary) MAX LEN is the max length of a sequence\n",
    "glove_model.add(layers.Flatten())\n",
    "glove_model.add(layers.Dense(3, activation='softmax'))\n",
    "glove_model.summary()\n",
    "\n",
    "\n",
    "#With the set_weights method we load the pre-trained embeddings in \n",
    "#the Embedding layer (here layer 0). By setting the trainable attribute to False, \n",
    "#we make sure not to change the pre-trained embeddings\n",
    "\n",
    "\n",
    "glove_model.layers[0].set_weights([emb_matrix])\n",
    "glove_model.layers[0].trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "class testing():\n",
    "    new_var = 5\n",
    "    \n",
    "    def print_n(self):\n",
    "        print(self.new_var)\n",
    "temp = testing()\n",
    "temp.print_n()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM CALLBACKS IN KERAS\n",
    "# THIS CALLBACK SAVES THE MODEL NOT NEEDED BUT IDK WHY I MADE THIS\n",
    "# custom callback\n",
    "class save_model_1(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,filepath):\n",
    "        self.paths = filepath\n",
    "    \n",
    "    min_loss = 10000000\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(logs['loss'])\n",
    "        print(self.paths)\n",
    "        if logs[\"loss\"] < self.min_loss:\n",
    "            min_loss = logs[\"loss\"]\n",
    "            temp_model = self.model\n",
    "            mem = []\n",
    "            self.model.save(self.paths)\n",
    "            #for i in temp_model.layers:\n",
    "            #    mem.append(i.trainable)\n",
    "            #    i.trainable = False\n",
    "            #\n",
    "            #for c,i in enumerate(temp_model):\n",
    "            #    i.trainable = mem[c]\n",
    "        \n",
    "save_model = save_model_1(\"E:\\Data Science\\Kaggle Competitions dataset\\Hackerearth gala\\dataset\\Models/timepass.h5\")\n",
    "#print(save_model.paths)\n",
    "print(type(save_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING LABEL ENCODER FOR TRANSFORMATIONS\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "enoder_dict = {}\n",
    "for i in df.columns:\n",
    "    df[i]= df[i].astype('category')\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df[i])\n",
    "    enoder_dict[i] = le\n",
    "    df[i] = le.transform(df[i])\n",
    "    \n",
    "# WILL DO THE INVERSE TRANSFORM AND BRING THE ORIGINAL WORD BACK\n",
    "for c,i in enumerate(df.columns):\n",
    "    x = enoder_dict[i]\n",
    "    print(x.inverse_transform(df[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pie chart\n",
    "\n",
    "temp = application_train[\"NAME_INCOME_TYPE\"].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.title(\"Income Type\")\n",
    "plt.pie(temp.values,labels = temp.index,autopct='%1.0f%%',)\n",
    "plt.legend(frameon=True, bbox_to_anchor=(1.5,0.8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make bar graphs\n",
    "temp = application_train[\"OCCUPATION_TYPE\"].value_counts()\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.title(\"Occupation of applicants\")\n",
    "sns.barplot(y = temp.index,x = temp.values,orient = 'h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE PLOT\n",
    "def kde_target(var_name, df):\n",
    "    \n",
    "    corr = df['TARGET'].corr(df[var_name])\n",
    "    \n",
    "    avg_repaid = df.ix[df['TARGET'] == 0, var_name].median()\n",
    "    avg_not_repaid = df.ix[df['TARGET'] == 1, var_name].median()\n",
    "    \n",
    "    plt.figure(figsize = (12, 6))\n",
    "    \n",
    "    sns.kdeplot(df.ix[df['TARGET'] == 0, var_name], label = 'TARGET == 0')\n",
    "    sns.kdeplot(df.ix[df['TARGET'] == 1, var_name], label = 'TARGET == 1')\n",
    "    \n",
    "    plt.xlabel(var_name); plt.ylabel('Density'); plt.title(f'%s Distribution {var_name}')\n",
    "    plt.legend();\n",
    "    \n",
    "    print(\"The correlation is \")\n",
    "    print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot colors link\n",
    "\n",
    "https://medium.com/@morganjonesartist/color-guide-to-seaborn-palettes-da849406d44f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\TheDarkAce\\Downloads\\small-subset.csv\",skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medallion</th>\n",
       "      <th>hack_license</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>surcharge</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89D227B655E5C82AECF13C3F540D4CF4</td>\n",
       "      <td>BA96DE419E711691B9445D6A6307C170</td>\n",
       "      <td>CMT</td>\n",
       "      <td>2013-01-01 15:11:48</td>\n",
       "      <td>CSH</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>\n",
       "      <td>9FD8F69F0804BDB5549F40E9DA1BE472</td>\n",
       "      <td>CMT</td>\n",
       "      <td>2013-01-06 00:18:35</td>\n",
       "      <td>CSH</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>\n",
       "      <td>9FD8F69F0804BDB5549F40E9DA1BE472</td>\n",
       "      <td>CMT</td>\n",
       "      <td>2013-01-05 18:49:41</td>\n",
       "      <td>CSH</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>\n",
       "      <td>51EE87E3205C985EF8431D850C786310</td>\n",
       "      <td>CMT</td>\n",
       "      <td>2013-01-07 23:54:15</td>\n",
       "      <td>CSH</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>\n",
       "      <td>51EE87E3205C985EF8431D850C786310</td>\n",
       "      <td>CMT</td>\n",
       "      <td>2013-01-07 23:25:03</td>\n",
       "      <td>CSH</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          medallion                      hack_license  \\\n",
       "0  89D227B655E5C82AECF13C3F540D4CF4  BA96DE419E711691B9445D6A6307C170   \n",
       "1  0BD7C8F5BA12B88E0B67BED28BEA73D8  9FD8F69F0804BDB5549F40E9DA1BE472   \n",
       "2  0BD7C8F5BA12B88E0B67BED28BEA73D8  9FD8F69F0804BDB5549F40E9DA1BE472   \n",
       "3  DFD2202EE08F7A8DC9A57B02ACB81FE2  51EE87E3205C985EF8431D850C786310   \n",
       "4  DFD2202EE08F7A8DC9A57B02ACB81FE2  51EE87E3205C985EF8431D850C786310   \n",
       "\n",
       "  vendor_id      pickup_datetime payment_type  fare_amount  surcharge  \\\n",
       "0       CMT  2013-01-01 15:11:48          CSH          6.5        0.0   \n",
       "1       CMT  2013-01-06 00:18:35          CSH          6.0        0.5   \n",
       "2       CMT  2013-01-05 18:49:41          CSH          5.5        1.0   \n",
       "3       CMT  2013-01-07 23:54:15          CSH          5.0        0.5   \n",
       "4       CMT  2013-01-07 23:25:03          CSH          9.5        0.5   \n",
       "\n",
       "   mta_tax  tip_amount  tolls_amount  total_amount  \n",
       "0      0.5         0.0           0.0           7.0  \n",
       "1      0.5         0.0           0.0           7.0  \n",
       "2      0.5         0.0           0.0           7.0  \n",
       "3      0.5         0.0           0.0           6.0  \n",
       "4      0.5         0.0           0.0          10.5  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = df.groupby('hack_license')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CSH', 'DIS', 'NOC', 'CRD'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.payment_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(df[i].isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1001, 11)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
